{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Hands-On NLP - Class 1: Foundations of Text Analysis\n",
    "\n",
    "**Date:** January 9, 2026\n",
    "\n",
    "**Goal:** This first notebook explores the fundamental questions of NLP:\n",
    "*   What is a character? (Unicode)\n",
    "*   What is a word? (Tokenization)\n",
    "*   How do words behave? (Zipf's Law, Type/Token Ratios)\n",
    "\n",
    "**Instructions:**\n",
    "1. Run the cells to see the results.\n",
    "2. Look for **üöß TODO:** markers. These are exercises for you.\n",
    "3. For \"Explain\" questions, write your answer in the markdown cell below the question.\n",
    "\n",
    "This notebook is designed to be completed **individually** in class (1h30)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "<span style=\"color:magenta\">Student name:</span>\n",
    "\n",
    "* üöß TODO: ... fill in your name ...\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup\n",
    "tqdm.pandas()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361548bd",
   "metadata": {},
   "source": [
    "## Download the Stackexchange Dataset provided by EleutherAI\n",
    "\n",
    "* original Github repository: https://github.com/EleutherAI/stackexchange-dataset \n",
    "* original data from: https://archive.org/details/stackexchange \n",
    "* small subset for this class: gerdes.fr/saclay/honlp/texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a1e81",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÊó†Ê≥ïÂêØÂä® Kernel \"Python 3 (ipykernel)\"„ÄÇ \n",
      "\u001b[1;31mÊü•Áúã Jupyter <a href='command:jupyter.viewOutput'>log</a>Ôºå‰∫ÜËß£Êõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ Unable to get resolved server information for google.colab:colab:2634a4e8-ccf8-4941-82fa-4e450744c78b"
     ]
    }
   ],
   "source": [
    "# Download and unzip the texts dataset if not already present\n",
    "\n",
    "DATA_DIR = Path(\"texts\")\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    print(\"Downloading texts.zip...\")\n",
    "    url = \"https://gerdes.fr/saclay/honlp/texts.zip\"\n",
    "    zip_path = \"texts.zip\"\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    \n",
    "    print(\"Extracting texts.zip...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    \n",
    "    # Clean up zip file\n",
    "    os.remove(zip_path)\n",
    "    print(\"Done!\")\n",
    "\n",
    "# Verify Data\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Could not find 'texts' directory at {DATA_DIR.absolute()}\")\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR.absolute()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Part 1: Text as Data (Unicode & Encoding)\n",
    "\n",
    "Before we process \"words\", we must handle \"characters\". Modern text is almost always Unicode (UTF-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### 1.1 Exploring Unicode\n",
    "\n",
    "**üöß TODO:**\n",
    "1. Why is UTF-8 the most common encoding on the web? What does UTF-8 stand for?\n",
    "2. Explore the text_sample below. What do you notice? If you're looking at this in VSCode, why do you see boxes around some characters?\n",
    "\n",
    "**Answer here:**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example string with diversity\n",
    "text_sample = \"Hello! üëã 123 √ø ‚Ç¨ƒï≈ÅËå∂Íù¢‡•Ä„Åå‚Ç¨,!‚â´‚ñ†‚úÖü§ó\\u200c\\u200e\\u3000\\xa0\\xad AŒë–êO„Äá0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9837b",
   "metadata": {},
   "source": [
    "Write a function `analyze_string(text)` that prints the name of each character in the text using `unicodedata.name()`.\n",
    "Each line should look like this:\n",
    "\n",
    "```\n",
    "'a' (U+0061): LATIN SMALL LETTER A\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99072c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_string(text):\n",
    "    print(f\"Analyzing: {text}\")\n",
    "    for char in text:\n",
    "        try:\n",
    "            # üöß TODO:\n",
    "            ... your code here ...\n",
    "        print(f\"'{char}' ... your code here ...\")\n",
    "\n",
    "analyze_string(text_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### 1.2 The Mystery Character\n",
    "\n",
    "**üöß TODO:**\n",
    "Identify the highest-numbered non-Chinese Unicode character in the following string.\n",
    "What is it called? Where did you find it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery_string = \"Hello üåç! This is a test with some weird chars: Ô∑Ω, üêç, and ü´Ä.\"\n",
    "\n",
    "analyze_string(mystery_string)\n",
    "\n",
    "# Code to find max explicitly:\n",
    "# üöß TODO:\n",
    "... your code here ...\n",
    "print(f\"\\nMax char by code point: ... your code here ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "*(Write your explanation here. Example: The character is the Earth emoji...)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Visual Lookalikes (Homoglyphs)\n",
    "\n",
    "**üöß TODO:**\n",
    "Look at the two groups of three characters in `text_sample` above that look similar.\n",
    "Run `analyze_string` on them. Why do they have different Unicode code points? How can this be a problem for text processing and internet security?\n",
    "\n",
    "**Answer:**\n",
    "... your answer here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Part 2: Loading & Visualizing Corpora\n",
    "\n",
    "We will load text files from the `texts` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPORA = [\"mythology\", \"woodworking\", \"robotics\", \"hsm\", \"health\", \"portuguese\"]\n",
    "\n",
    "corpora_text = {}\n",
    "stats = []\n",
    "\n",
    "# üöß TODO: Complete the code to load the data\n",
    "# Create a dictionary `corpora_text` mapping corpus name -> full string content\n",
    "# And a list `stats` with info per corpus\n",
    "\n",
    "for corpus in tqdm(CORPORA):\n",
    "    corpus_path = DATA_DIR / corpus\n",
    "\n",
    "    texts = []\n",
    "    files = list(corpus_path.glob(\"*.txt\"))\n",
    "    ... your code here ...\n",
    "    stats.append({\n",
    "        \"corpus\": corpus, \n",
    "        \"files_n\": len(files), \n",
    "        \"chars_n\": len(full_text)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(stats).set_index(\"corpus\")\n",
    "df['text'] = ... your code here ...\n",
    "...\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### 2.1 Character, Type, and Token Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Visualize character counts per corpus (histogram)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=... your code here ...)\n",
    "plt.title(\"Total Characters per Corpus\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Observation:**\n",
    "Look at the plot above. Is our dataset balanced?\n",
    "\n",
    "*   **No.** Some corpora are much larger than others (e.g., maybe \"woodworking\" vs \"hsm\").\n",
    "*   **Consequence:** Comparing raw counts (like \"number of unique characters\") directly is unfair. We need normalized metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### 2.2 Character Frequency Analysis\n",
    "\n",
    "**Why do we do this?**\n",
    "*   To identify the **language** (Portuguese will have `√£`, `√ß`).\n",
    "*   To spot **artifacts** (encoding errors, weird symbols).\n",
    "*   To fingerprint the **domain** (Math symbols in Robotics? Emojis in informal text?).\n",
    "\n",
    "Let's look at the distribution of characters in \"mythology\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Create a frequency histogram for the most frequent characters in mythology\n",
    "# Then create a log-log plot to see if the characters follow Zipf's law.\n",
    "# Provide your analysis below.\n",
    "\n",
    "# Solution:\n",
    "if \"mythology\" in df.index:\n",
    "    myth_text = df.loc[\"mythology\", \"text\"]\n",
    "    ... your code here ...\n",
    "\n",
    "    # Plot top 30\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(... your code here ...)\n",
    "    plt.title(\"Top 30 Characters in Mythology\")\n",
    "    plt.show()\n",
    "\n",
    "    # Log-Log Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.loglog(... your code here ...)\n",
    "    plt.title(\"Character Distribution (Log-Log) - Zipfian?\")\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Observation:**\n",
    "### üöß TODO:\n",
    "... your observations here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### 2.3 Character Richness (Diversity)\n",
    "\n",
    "Does every corpus use the same variety of characters?\n",
    "\n",
    "**Method 1: The Naive Approach**\n",
    "Calculate `char_types_n` (number of unique characters) and divide by `chars_n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Add 'char_types_n' and 'char_type_ratio' column\n",
    "df[\"char_types_n\"] = ... your code here ...\n",
    "df[\"char_type_ratio\"] = ... your code here ...\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax1 = sns.barplot(x=df.index, y=\"char_type_ratio\", data=df, hue=\"corpus\")\n",
    "ax1.yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=2))\n",
    "plt.title(\"Naive Diversity Ratio (Types / Total Chars)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Wait a minute!** üõë\n",
    "\n",
    "Look at the previous results. The smallest corpus often looks \"richest\".\n",
    "\n",
    "**The Problem:**\n",
    "As you read more text, finding *new* characters (or words) becomes harder.\n",
    "Type-Token Ratio (TTR) naturally decreases as text length increases.\n",
    "Comparing TTR on corpora of different sizes is **not fair**.\n",
    "\n",
    "**Method 2: Fixed-Size Window (The Correct Way)**\n",
    "We should compare the richness on the **same amount of text** (e.g., the first 10,000 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Calculate diversity averaged over sliding windows\n",
    "N = 10000\n",
    "\n",
    "def get_sliding_diversity(text, window_size):\n",
    "    if len(text) < window_size:\n",
    "        if len(text) == 0: return 0.0\n",
    "        return len(set(text)) / len(text)\n",
    "\n",
    "    # We take non-overlapping chunks for efficiency (Mean TTR)\n",
    "    ratios = []\n",
    "    for i in range(0, len(text) - window_size + 1, window_size):\n",
    "        chunk = ... your code here ...\n",
    "    return np.mean(ratios)\n",
    "\n",
    "df[\"fixed_window_diversity\"] = df[\"text\"].apply(lambda t: get_sliding_diversity(t, N))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax2 = sns.barplot(x=df.index, y=\"fixed_window_diversity\", data=df, hue=\"corpus\")\n",
    "ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=2))\n",
    "plt.title(f\"Fair Diversity Ratio (Avg over {N}-char windows)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Question:** Now compare the \"Naive\" vs \"Fixed-Window\" plots. \n",
    "1. Did the ranking change?\n",
    "2. Provide a hand-wavy explanation of the two \"diversity winner\" corpora. You may have to look into the texts to answer this.\n",
    "\n",
    "**Answer:**\n",
    "... your observations here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfG",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Part 3: Tokenization\n",
    "\n",
    "**Goal:** Split the text into meaningful units (words).\n",
    "\n",
    "We will compare methods:\n",
    "1.  **Simple Split:** `text.split()` (Splits on whitespace).\n",
    "2.  **Regex Split:** `re.split(r'\\W+', text)` (Splits on non-alphanumeric).\n",
    "    *   *Variant:* `re.split(r'(\\W+)', text)` (Keeps delimiters).\n",
    "3.  **Linguistic Split:** `nltk.word_tokenize` (Smart rules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Wait‚Äîwhat?! I can't believe it's 2026...\"\n",
    "print(f\"Original: {sentence}\\n\")\n",
    "\n",
    "# Method 1: Simple Split\n",
    "print(\"1. text.split():\")\n",
    "print(sentence.split())\n",
    "# Problem: Punctuation sticks to words (\"2026...\" is one token)\n",
    "\n",
    "# Method 2a: Regex Split (\\W+) - eats punctuation\n",
    "print(\"\\n2a. Regex split (\\W+):\")\n",
    "print(re.split(r'\\W+', sentence))\n",
    "# Problem: \"can't\" becomes \"can\", \"t\". Punctuation is gone.\n",
    "\n",
    "# Method 2b: Regex Split with Capturing Group ((\\W+)) - keeps punctuation\n",
    "print(\"\\n2b. Regex split with capturing ((\\W+)):\")\n",
    "# By wrapping the separator pattern in parentheses, split() returns the separators too!\n",
    "print(re.split(r'(\\W+)', sentence))\n",
    "# Better: We keep the punctuation, but it's treated as separate tokens.\n",
    "\n",
    "# Method 3: NLTK\n",
    "print(\"\\n3. NLTK:\")\n",
    "try:\n",
    "    print(nltk.word_tokenize(sentence))\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    print(nltk.word_tokenize(sentence))\n",
    "# Advantage: \"n't\" is handled, punctuation is preserved as separate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBYS",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### üöß TODO:\n",
    "**Conclusion:**\n",
    "*   **Split** ... your observations here ...\n",
    "*   **Regex** ... your observations here ...\n",
    "*   **NLTK** (and Spacy) ... your observations here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aLJB",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### 3.1 Subword Tokenization (BPE)\n",
    "\n",
    "Large Language Models use **Byte-Pair Encoding (BPE)** to fix the \"Out Of Vocabulary\" (OOV) problem.\n",
    "\n",
    "**The Goal:** Algorithmically find the best subwords to represent a text.\n",
    "\n",
    "**Step 1: Preparation**\n",
    "We start with a raw string, tokenize it into words, and then split each word into characters.\n",
    "We append `</w>` to the end of each word to mark the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose invariant roots (no spelling changes) to show clean merges.\n",
    "# walk: 4 forms * 3 = 12 times. jump: 12 times.\n",
    "# suffixes: ing, s, ed appear 6 times each.\n",
    "# Result: 'walk' and 'jump' should become tokens, then suffixes attach.\n",
    "raw_text = \"walk walking walks walked \" * 4 + \"jump jumping jumps jumped \" * 3\n",
    "\n",
    "def get_vocab(text):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    # Use regex to split words and keep punctuation separate\n",
    "    words = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "    for word in words:\n",
    "        # Add spaces between chars and </w> at the end\n",
    "        vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(raw_text)\n",
    "print(\"Initial Vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xXTn",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Step 2: Find the Best Pair**\n",
    "\n",
    "**üöß TODO 1:** Write `get_stats(vocab)` to find one of the most frequent pairs of adjacent symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVT",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "pairs_step1 = get_stats(vocab)\n",
    "best_pair = max(pairs_step1, key=pairs_step1.get)\n",
    "print(f\"On of the most frequent pair: {best_pair} (Count: {pairs_step1[best_pair]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pHFh",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Step 3: Merge and Iterate**\n",
    "\n",
    "**üöß TODO 2:** Write `merge_vocab(pair, v_in)` and run a loop for **N merges**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "# üöß TODO: Run 12 merges to see meaningful subwords\n",
    "current_vocab = vocab.copy()\n",
    "print(\"Starting BPE Merges...\\n\")\n",
    "\n",
    "steps_log = []\n",
    "\n",
    "for i in range(12):\n",
    "    pairs_iter = get_stats(current_vocab)\n",
    "    if not pairs_iter:\n",
    "        break\n",
    "    ... your code here ...\n",
    "    steps_log.append(f\"Step {i+1}: Merged {best_iter}\")\n",
    "    print(f\"Step {i+1}: Merged {best_iter}\")\n",
    "\n",
    "print(\"\\nResulting Vocabulary:\")\n",
    "# Display nicely\n",
    "vocab_table = [{\"Tokenized Word\": k, \"Frequency\": v} for k, v in current_vocab.items()]\n",
    "print(pd.DataFrame(vocab_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Step 4: Real World Example**\n",
    "\n",
    "**üöß TODO 3:** Train BPE on the 'woodworking' corpus!\n",
    "1. Build initial vocab from `woodworking` text (first 10k chars).\n",
    "2. Run 100 merges.\n",
    "3. Show some words that are ONE token (e.g. \"wood\") vs words that are SPLIT (e.g. \"un-believ-able\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get Text\n",
    "train_text = df.loc[\"woodworking\", \"text\"][:100000] # Small subset for speed\n",
    "\n",
    "# 2. Build Vocab\n",
    "wood_vocab = get_vocab(train_text)\n",
    "\n",
    "# 3. Train (2000 merges)\n",
    "for _i in range(2000):\n",
    "    pairs_train = get_stats(wood_vocab)\n",
    "    if not pairs_train: break\n",
    "    best_train = ... your code here ...\n",
    "    wood_vocab = merge_vocab(best_train, wood_vocab)\n",
    "\n",
    "# 4. Analysis\n",
    "print(\"Training done. Analyzing results...\")\n",
    "results = []\n",
    "\n",
    "for word_seq in wood_vocab:\n",
    "    wood_tokens = word_seq.split()\n",
    "    original_word = \"\".join(wood_tokens).replace(\"</w>\", \"\")\n",
    "    # A word is a \"Full Token\" if it's a single subword (+ end marker)\n",
    "    # i.e., either ['word</w>'] or ['X', '</w>']\n",
    "    is_full_token = (len(wood_tokens) == 1) or (len(wood_tokens) == 2 and wood_tokens[-1] == '</w>')\n",
    "    if is_full_token:\n",
    "        results.append({\"Word\": original_word, \"Type\": \"Full Token\", \"Sequence\": str(wood_tokens)})\n",
    "    else:\n",
    "         results.append({\"Word\": original_word, \"Type\": \"Split\", \"Sequence\": str(wood_tokens)})\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\nAnalysis ({len(results)} total words):\")\n",
    "\n",
    "print(\"\\n--- Examples of Full Tokens (Learned) ---\")\n",
    "print(df_results[df_results[\"Type\"] == \"Full Token\"].head(10))\n",
    "\n",
    "print(\"\\n--- Examples of Split Words (Morphology/Rare) ---\")\n",
    "print(df_results[df_results[\"Type\"] == \"Split\"].head(10))\n",
    "\n",
    "print(f\"\\n--- Vocabulary Statistics ---\")\n",
    "print(f\"Total unique words in corpus: {len(results)}\")\n",
    "print(f\"Full Tokens (single subword): {len(df_results[df_results['Type'] == 'Full Token'])}\")\n",
    "print(f\"Split Words (multiple subwords): {len(df_results[df_results['Type'] == 'Split'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TXez",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Part 4: Token Statistics (Zipf's Law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK to tokenize 'woodworking'\n",
    "wood_text = df.loc[\"woodworking\", \"text\"]\n",
    "\n",
    "# üöß TODO: Tokenize (you can limit the size)\n",
    "tokens = nltk. ... your code here ...\n",
    "\n",
    "# üöß TODO: Plot Zipf\n",
    "counts = Counter(tokens)\n",
    "freqs = ... your code here ...\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.loglog(range(1, len(freqs)+1), freqs, marker=\".\")\n",
    "plt.title(\"Token Frequency (Zipf's Law) - Woodworking\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCnT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Part 5: Conclusion\n",
    "\n",
    "You have successfully explored the atomic units of NLP!\n",
    "\n",
    "**üéâ CONGRATULATIONS! üéâ**\n",
    "\n",
    "You survived Unicode (barely), tamed the Tokenizers, and validated Zipf's Law without needing a lawyer!\n",
    "\n",
    "Go treat yourself to a `\\u1F355`!\n",
    "\n",
    "One last TODO: How to find out what to get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlCL",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this looks easy:\n",
    "print(\"\\u1F355\") \n",
    "# but wait: it doesn't show what to get:\n",
    "# Why? Python has two types of Unicode escape sequences:\n",
    "\n",
    "# 1. \\uXXXX (4 digits, 16-bit) -> For Basic Multilingual Plane (BMP) only.\n",
    "#    \"\\u1F35\" is read as '·Ωë' (Greek Dasia), followed by the literal \"5\".\n",
    "# 2. \\UXXXXXXXX (8 digits, 32-bit) -> For any Unicode character (including Emoji).\n",
    "# and \\u1F355 requires 8 digits.\n",
    "print(\"\\U0001F355\")\n",
    "\n",
    "# 3. By Name\n",
    "print(\"\\N{CUCUMBER}\") # that's good for health but not the right name of the treat. \n",
    "# TODO: correct it to the right treat to get now. But how to find the name of \\U0001F355?\n",
    "\n",
    "# How to find the name?\n",
    "treat_char = \"\\U0001F355\"\n",
    "print(f\"Name of {treat_char} is: TODO: ... your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
