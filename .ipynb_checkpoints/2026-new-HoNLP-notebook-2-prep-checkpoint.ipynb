{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a449a29b",
   "metadata": {},
   "source": [
    "# Hands-On NLP - Class 2: Sparse Representations & Traditional ML\n",
    "\n",
    "**Goal:** Transform text into numerical vectors and build text classifiers.\n",
    "\n",
    "## Notebook Outline\n",
    "\n",
    "| Part | Topic | Description |\n",
    "|------|-------|-------------|\n",
    "| **1** | Loading the Dataset | Load StackExchange texts from 6 categories |\n",
    "| **2** | Manual Vectorization | Build intuition by counting keywords by hand |\n",
    "| **3** | Sklearn Vectorization | Use `CountVectorizer` and `TfidfVectorizer` |\n",
    "| **4** | Classification | Train Naive Bayes on TF-IDF features |\n",
    "| **5** | Evaluation & Error Analysis | Confusion matrices, classification reports |\n",
    "| **6** | Comparing Classifiers | NB vs Logistic Regression vs SVM |\n",
    "| **7** | N-gram Experiments | Unigrams vs bigrams vs combined |\n",
    "| **8** | Feature Importance | Which words predict each category? + Word Clouds |\n",
    "| **9** | Hyperparameter Tuning | Effect of `max_features`, `min_df`, and regularization |\n",
    "| **10** | Cross-Validation | Robust evaluation with k-fold CV |\n",
    "\n",
    "**Key Concepts:**\n",
    "- Bag of Words (BoW) and TF-IDF representations\n",
    "- Sparse matrix representations for text\n",
    "- Multi-class classification with traditional ML\n",
    "- Model comparison and hyperparameter exploration\n",
    "- Cross-validation for reliable performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "MJUe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T12:47:03.210164Z",
     "start_time": "2026-01-15T12:47:02.629424Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è wordcloud not installed. Run: pip install wordcloud\")\n",
    "# Setup\n",
    "tqdm.pandas()\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "pd.set_option(\"display.precision\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c5fc7",
   "metadata": {},
   "source": [
    "## Part 1: Loading the Dataset\n",
    "\n",
    "> ‚ö†Ô∏è **Important:** This notebook requires the `texts/` folder containing the StackExchange dataset.  \n",
    "> Make sure to place this notebook **in the same folder as Notebook 1**, where you extracted the `texts/` folder.  \n",
    "> If you see a \"file not found\" error, check that the folder structure looks like:\n",
    "> ```\n",
    "> your_folder/\n",
    "> ‚îú‚îÄ‚îÄ 2026-new-HoNLP-notebook-1-....ipynb\n",
    "> ‚îú‚îÄ‚îÄ 2026-new-HoNLP-notebook-2-....ipynb  ‚Üê this notebook\n",
    "> ‚îî‚îÄ‚îÄ texts/\n",
    ">     ‚îú‚îÄ‚îÄ health/\n",
    ">     ‚îú‚îÄ‚îÄ mythology/\n",
    ">     ‚îî‚îÄ‚îÄ ...\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bkHC",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T12:47:16.424742Z",
     "start_time": "2026-01-15T12:47:12.340346Z"
    }
   },
   "outputs": [],
   "source": [
    "# StackExchange Dataset\n",
    "# We load the texts from the 'texts' folder as in previous weeks\n",
    "DATA_DIR = Path(\"texts\")\n",
    "CORPORA = [\"mythology\", \"woodworking\", \"robotics\", \"hsm\", \"health\", \"portuguese\"]\n",
    "\n",
    "data = []\n",
    "# Load data into a list of dictionaries\n",
    "# Iterate over corpora, read files, append to data list\n",
    "\n",
    "# Solution:\n",
    "for cat_id, corpus in enumerate(tqdm(CORPORA)):\n",
    "    corpus_path = DATA_DIR / corpus\n",
    "    if not corpus_path.exists():\n",
    "        continue\n",
    "    for text_file in corpus_path.glob(\"*.txt\"):\n",
    "        with open(text_file, \"r\") as f:\n",
    "            text = f.read()\n",
    "        data.append({\n",
    "            \"text\": text,\n",
    "            \"category\": corpus,\n",
    "            \"cat_id\": cat_id\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(data)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lEQa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T12:47:20.854209Z",
     "start_time": "2026-01-15T12:47:20.800380Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "# Shuffle\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a96acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Overview: How balanced is our corpus?\n",
    "print(\"Documents per category:\")\n",
    "print(df[\"category\"].value_counts())\n",
    "print(f\"\\nTotal documents: {len(df)}\")\n",
    "print(f\"Average text length: {df['text'].str.len().mean():.0f} characters\")\n",
    "\n",
    "# Add text length column for visualization\n",
    "df[\"text_len\"] = df[\"text\"].str.len()\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart of document counts\n",
    "df[\"category\"].value_counts().plot(kind=\"bar\", ax=axes[0], color=plt.cm.tab10.colors[:6])\n",
    "axes[0].set_title(\"Number of Documents per Category\")\n",
    "axes[0].set_xlabel(\"Category\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# üöß TODO: Box plot of text lengths by category\n",
    "df.boxplot(... your code here ..., grid=False)\n",
    "axes[1].set_title(\"Text Length Distribution by Category\")\n",
    "axes[1].set_xlabel(\"Category\")\n",
    "axes[1].set_ylabel(\"Characters\")\n",
    "plt.suptitle(\"\")  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d72ac3",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Dataset Imbalance Alert!\n",
    "\n",
    "**Our dataset is heavily imbalanced:** Robotics has ~40,000 documents while other categories have only ~1,000-3,000 each!\n",
    "\n",
    "**Why this matters:**\n",
    "- A model predicting \"robotics\" for everything would get ~80% accuracy!\n",
    "- **Accuracy can be misleading** when classes are imbalanced\n",
    "\n",
    "**Better metrics for imbalanced data:**\n",
    "- **Macro F1-score** ‚Äî Average F1 across classes (treats all classes equally)\n",
    "- **Per-class Precision/Recall** ‚Äî See performance on minority classes\n",
    "- **Confusion Matrix** ‚Äî Visualize where errors occur\n",
    "\n",
    "üëâ **Best practice:** Always check the classification report's F1-scores, not just accuracy! In this notebook, our models perform so well (~98-99%) that accuracy and Macro F1 are very close ‚Äî but for harder tasks or worse models, the gap can be huge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef7fd7",
   "metadata": {},
   "source": [
    "## Part 2: From Strings to Numbers (Intuition)\n",
    "\n",
    "Before using powerful libraries, let's build a vectorizer \"by hand\" to understand what's happening.\n",
    "\n",
    "We will pick a few keywords and count their occurrence in each document.\n",
    "\n",
    "**üöß TODO:** Create a DataFrame where each column is a keyword count for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Count keyword occurrences in each document\n",
    "# Small vocabulary of interest\n",
    "manual_keywords = \"myth,wood,robot,history,science,mathematics,health,portuguese\".split(\",\")\n",
    "\n",
    "# Solution:\n",
    "# Create a DataFrame of counts\n",
    "wc_df = pd.DataFrame(index=df.index)\n",
    "for w in manual_keywords:\n",
    "    # Case insensitive count? For simplicity, we stick to data as is or simple lower().\n",
    "    # Let's check how many times each keyword appears in each text\n",
    "    wc_df[w] = ... your code here ...\n",
    "\n",
    "print(\"Manual Bag of Words (First 5 docs):\")\n",
    "print(wc_df.head())\n",
    "\n",
    "# Add the labels back for visualization\n",
    "wc_df[\"label\"] = df[\"category\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde49ce0",
   "metadata": {},
   "source": [
    "### 2.1 Visualizing High-Dimensional Data (PCA)\n",
    "\n",
    "Our `wc_df` has 8 dimensions (one for each keyword). We can't see 8D!\n",
    "\n",
    "**What is PCA?**  \n",
    "PCA (Principal Component Analysis) finds the directions of maximum variance in your data and projects it onto fewer dimensions.\n",
    "\n",
    "üéØ **Intuition:** Imagine shining a flashlight on a 3D object and looking at its 2D shadow on the wall. PCA finds the best angle to shine the light so the shadow preserves as much shape information as possible.\n",
    "\n",
    "```\n",
    "8D data (8 keyword counts)\n",
    "    ‚îÇ\n",
    "    ‚ñº  PCA\n",
    "2D data (PC1, PC2) ‚îÄ‚îÄ‚ñ∫ Can plot on screen!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop label for PCA\n",
    "X_manual_pca = wc_df.drop(columns=[\"label\"])\n",
    "y_manual_pca = wc_df[\"label\"]\n",
    "\n",
    "# üöß TODO: Simple PCA to 2 components\n",
    "pca = ... your code here ...\n",
    "X_pca_2d = ... your code here ...\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca_2d[:, 0], y=X_pca_2d[:, 1], hue=y_manual_pca, alpha=0.6, palette=\"tab10\")\n",
    "plt.title(\"PCA of Manual Counts (8 keywords -> 2D)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1786aa3",
   "metadata": {},
   "source": [
    "### ü§î Why do only Robotics and Woodworking stand out?\n",
    "\n",
    "Look at the PCA plot above: **robotics** (orange, spread along PC1) and **woodworking** (blue, spread along PC2) are clearly separated, but all other categories are clustered at the origin!\n",
    "\n",
    "### üöß TODO: Find the reason. Explain what you've learned.\n",
    "**The reason:** \n",
    "\n",
    "... your explanation here ...\n",
    "\n",
    "**Key insight:** \n",
    "\n",
    "... your insight here ...\n",
    "\n",
    "**Lesson:** Manual feature selection is brittle! This is why we need **automatic vectorization** (CountVectorizer, TF-IDF) that considers ALL words in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030dde6f",
   "metadata": {},
   "source": [
    "## Part 3: Professional Vectorization (Sklearn)\n",
    "\n",
    "Counting by hand is slow and misses thousands of words.\n",
    "We use `scikit-learn` for efficient Bag of Words (BoW) & TF-IDF.\n",
    "\n",
    "### 3.1 CountVectorizer (Bag of Words)\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "```\n",
    "Document: \"the robot builds robots\"\n",
    "            ‚Üì\n",
    "Vocabulary: [builds, robot, robots, the]\n",
    "            ‚Üì\n",
    "Vector:     [  1,      1,      1,     1  ]  (counts)\n",
    "```\n",
    "\n",
    "Each document becomes a **sparse vector** of word counts.  \n",
    "\"Sparse\" = mostly zeros (a document uses only ~100 words out of 50,000 vocabulary).\n",
    "\n",
    "**üöß TODO:**\n",
    "Use `CountVectorizer` to turn the text into a document-term matrix.\n",
    "Limit to `max_features=5000` to keep it manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer(... your code here ..., stop_words=\"english\")\n",
    "X_bow = vectorizer. ... your code here ...\n",
    "\n",
    "print(f\"Shape of BoW matrix: {X_bow.shape}\")\n",
    "print(f\"Feature names (first 10): {vectorizer.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c19362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's peek inside the sparse matrix!\n",
    "# Sparse matrices store only non-zero values (memory efficient)\n",
    "\n",
    "print(\"Type:\", type(X_bow))\n",
    "print(\"Stored as sparse matrix with only\", X_bow.nnz, \"non-zero values\")\n",
    "print(f\"If dense: {X_bow.shape[0] * X_bow.shape[1]:,} values ({X_bow.shape[0]}√ó{X_bow.shape[1]})\")\n",
    "print(f\"Sparsity: {100 * (1 - X_bow.nnz / (X_bow.shape[0] * X_bow.shape[1])):.1f}% zeros\\n\")\n",
    "\n",
    "# Show a tiny slice as a dense array\n",
    "print(\"First 3 documents, first 10 words:\")\n",
    "print(\"Words:\", vectorizer.get_feature_names_out()[:10])\n",
    "print(X_bow[:3, :10].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a3caa",
   "metadata": {},
   "source": [
    "### 3.2 TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "Words that appear everywhere (like \"the\", \"is\") are less informative.\n",
    "TF-IDF downweights common words and upweights distinctive ones.\n",
    "\n",
    "**The Formula:**\n",
    "$$ \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\log\\left(\\frac{N}{\\text{DF}(t)}\\right) $$\n",
    "\n",
    "**Concrete Example:**\n",
    "\n",
    "| Term | Doc1 Count (TF) | Appears in N docs (DF) | Total docs (N) | IDF = log(N/DF) | TF-IDF |\n",
    "|------|-----------------|------------------------|----------------|-----------------|--------|\n",
    "| \"the\" | 10 | 1000 | 1000 | log(1) = 0 | 10 √ó 0 = **0** |\n",
    "| \"robot\" | 5 | 50 | 1000 | log(20) ‚âà 3 | 5 √ó 3 = **15** |\n",
    "| \"arduino\" | 2 | 10 | 1000 | log(100) ‚âà 4.6 | 2 √ó 4.6 = **9.2** |\n",
    "\n",
    "üéØ **Key insight:** \"the\" appears everywhere ‚Üí IDF ‚âà 0 ‚Üí TF-IDF ‚âà 0 (ignored!)  \n",
    "   Rare domain words get high scores.\n",
    "\n",
    "**üöß TODO:** Use `TfidfVectorizer` instead of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Initialize and fit TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(... your code here ..., stop_words=\"english\")\n",
    "X_tfidf = tfidf_vec. ... your code here ...\n",
    "\n",
    "print(f\"Shape of TF-IDF matrix: {X_tfidf.shape}\") \n",
    "# should be something like \n",
    "# Shape of TF-IDF matrix: (52683, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c0125",
   "metadata": {},
   "source": [
    "### 3.3 Visualizing TF-IDF Features with PCA\n",
    "\n",
    "Let's visualize our high-dimensional TF-IDF features in 2D using **Principal Component Analysis (PCA)**.\n",
    "\n",
    "**What is PCA?**\n",
    "- Reduces dimensions while preserving variance\n",
    "- Projects data onto orthogonal axes that capture maximum variance\n",
    "- Helps visualize high-dimensional data\n",
    "\n",
    "We'll reduce from 5000 dimensions to 2 dimensions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee21db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# üöß TODO: Apply PCA to reduce to 2 dimensions\n",
    "pca = ... your code here ...\n",
    "X_pca = pca. ... your code here ...\n",
    "\n",
    "# Create scatter plot colored by category\n",
    "plt.figure(figsize=(12, 8))\n",
    "categories = df[\"category\"].unique()\n",
    "colors = plt.cm.tab10(range(len(categories)))\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    mask = df[\"category\"] == category\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                c=[colors[i]], label=category, alpha=0.6, s=50)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.title('TF-IDF Features Visualized with PCA')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total variance explained by 2 components: {sum(pca.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61f7da",
   "metadata": {},
   "source": [
    "### üîç Observations from the TF-IDF PCA Plot\n",
    "\n",
    "**What stands out:**\n",
    "\n",
    "1. **Portuguese is highly distinctive** ‚Äî The green cluster forms a clear diagonal line, completely separated from all other categories. This makes sense: Portuguese texts use an entirely different language vocabulary!\n",
    "\n",
    "2. **Robotics forms its own cluster** ‚Äî The orange cluster spreads widely on the left side, indicating robotics has distinctive technical vocabulary (sensors, motors, Arduino, etc.)\n",
    "\n",
    "3. **Other categories overlap** ‚Äî Woodworking, health, hsm (history of science/math), and mythology are clustered together near the center. Their vocabularies share more common English words.\n",
    "\n",
    "4. **Low variance explained (~3%)** ‚Äî This is typical for text data! TF-IDF produces very high-dimensional sparse vectors where variance is spread across thousands of dimensions. PCA captures only a small fraction, yet the clustering is still visible.\n",
    "\n",
    "**Key insight:** Even with just 2 components explaining ~3% of variance, we can already see category structure. This suggests TF-IDF features will work well for classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove robotics and portuguese to see if remaining categories are separable\n",
    "df_subset = df[~df[\"category\"].isin([\"robotics\", \"portuguese\"])].copy()\n",
    "print(f\"Remaining documents: {len(df_subset)} (removed robotics and portuguese)\")\n",
    "print(f\"Categories: {df_subset['category'].unique()}\")\n",
    "\n",
    "# üöß TODO: Re-fit TF-IDF on the subset\n",
    "tfidf_subset = TfidfVectorizer(... your code here ..., stop_words=\"english\")\n",
    "X_tfidf_subset = tfidf_subset. ... your code here ...\n",
    "\n",
    "# Apply PCA\n",
    "pca_subset = ... your code here ...\n",
    "X_pca_subset = ... your code here ...\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "categories_subset = ... your code here ...\n",
    "colors_subset = ... your code here ...\n",
    "\n",
    "for i, category in enumerate(categories_subset):\n",
    "    mask = df_subset[\"category\"] == category\n",
    "    plt.scatter(... your code here ...)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_subset.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca_subset.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.title('TF-IDF PCA without Robotics & Portuguese')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total variance explained by 2 components: {sum(pca_subset.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958c02b",
   "metadata": {},
   "source": [
    "### üîç Observations after Removing Outlier Categories\n",
    "\n",
    "### üöß TODO: \n",
    "\n",
    "**What changed:**\n",
    "... your observations here ...\n",
    "\n",
    "**Key insight:** Removing \"easy\" categories ... your insight here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abff299",
   "metadata": {},
   "source": [
    "## Part 4: Classification\n",
    "\n",
    "Now we use our TF-IDF vectors to train a classifier!\n",
    "\n",
    "**Multinomial Naive Bayes:**  \n",
    "A simple, fast classifier that works well for text. It assumes:\n",
    "- Words are independent (the \"naive\" assumption)\n",
    "- Word counts follow a multinomial distribution\n",
    "\n",
    "```\n",
    "Training: Learn P(word | category) from labeled examples\n",
    "Prediction: P(category | document) ‚àù P(category) √ó ‚àè P(word | category)\n",
    "```\n",
    "\n",
    "**üöß TODO:**\n",
    "1. Split data into Train/Test (80/20)\n",
    "2. Train the model on TF-IDF features\n",
    "3. Evaluate on held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"cat_id\"]\n",
    "\n",
    "# üöß TODO: Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ... your code here ...\n",
    ")\n",
    "\n",
    "# üöß TODO: Train Model\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4b3e7",
   "metadata": {},
   "source": [
    "## Part 5: Evaluation & Error Analysis\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Precision:** Of all predicted \"mythology\", how many were actually mythology?\n",
    "- **Recall:** Of all actual mythology texts, how many did we find?\n",
    "- **F1-Score:** Harmonic mean of precision & recall (balanced measure)\n",
    "\n",
    "**Confusion Matrix:** Shows where the model gets confused.\n",
    "```\n",
    "                 Predicted\n",
    "              myth  wood  robot\n",
    "Actual myth   [45]   2     3     ‚Üê 45 correct, 5 confused with others\n",
    "       wood    1   [38]    1\n",
    "       robot   0     2   [42]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Print classification report\n",
    "print(classification_report(... your code here ...))\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: With imbalanced data, focus on:\n",
    "# - Macro avg F1: treats all classes equally (not weighted by support)\n",
    "# - Per-class F1: check performance on minority classes (mythology, woodworking, etc.)\n",
    "# - Don't be fooled by high accuracy ‚Äî it's inflated by the robotics majority class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Plot Confusion Matrix\n",
    "cm = confusion_matrix(... your code here ...)\n",
    "disp = ConfusionMatrixDisplay(... your code here ...)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(ax=ax, cmap=\"Blues\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Confusion Matrix (percentages per true class)\n",
    "# This shows RECALL for each class: what % of actual category X were correctly predicted?\n",
    "# Useful when corpus sizes are imbalanced!\n",
    "\n",
    "cm_normalized = confusion_matrix(... your code here ... , normalize='true')  # normalize by row (true labels)\n",
    "disp_norm = ConfusionMatrixDisplay(... your code here ...)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp_norm.plot(ax=ax, cmap=\"Blues\", values_format=\".1%\")\n",
    "plt.title(\"Normalized Confusion Matrix (Recall per Class)\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Each row sums to 100% (or 1.0)\")\n",
    "print(\"Diagonal = Recall (sensitivity) for each class\")\n",
    "print(\"Off-diagonal = where errors go for each true class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Error Analysis: Let's find where the model fails\n",
    "\n",
    "# First, let's see overall accuracy\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"Overall Accuracy: {accuracy:.1%}\")\n",
    "print(f\"‚ö†Ô∏è  Remember: accuracy is inflated by the robotics majority class!\")\n",
    "print(f\"    Check the macro F1-score above for a fairer evaluation.\\n\")\n",
    "print(f\"Errors: {(y_pred != y_test).sum()} out of {len(y_test)} test samples\\n\")\n",
    "\n",
    "# Find misclassified examples\n",
    "# We need to track which original documents ended up in test set\n",
    "df_test = df.iloc[y_test.index].copy()\n",
    "df_test[\"predicted\"] = y_pred\n",
    "### üöß TODO:\n",
    "df_test[\"correct\"] = ... your code here ...\n",
    "\n",
    "# Show one error\n",
    "errors_df = df_test[~df_test[\"correct\"]]\n",
    "\n",
    "if len(errors_df) > 0:\n",
    "    sample = errors_df.iloc[0]\n",
    "    true_cat = CORPORA[sample[\"cat_id\"]]\n",
    "    pred_cat = CORPORA[sample[\"predicted\"]]\n",
    "    \n",
    "    print(f\"‚ùå MISCLASSIFIED EXAMPLE:\")\n",
    "    print(f\"   True category:      {true_cat}\")\n",
    "    print(f\"   Predicted category: {pred_cat}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(sample[\"text\"][:500] + \"...\")\n",
    "else:\n",
    "    print(\"üéâ Perfect! No errors found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba44f4",
   "metadata": {},
   "source": [
    "## Part 6: Comparing Classifiers\n",
    "\n",
    "Different classifiers have different strengths. Let's compare:\n",
    "- **Naive Bayes** (fast, probabilistic)\n",
    "- **Logistic Regression** (linear, interpretable)\n",
    "- **Support Vector Machine** (powerful for high-dimensional sparse data)\n",
    "\n",
    "**üöß TODO:** Run multiple classifiers and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Compare multiple classifiers (NB, LogReg, SVM)\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "# Solution:\n",
    "def evaluate_classifier(name, clf, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate a classifier, return metrics.\"\"\"\n",
    "    start = time.time()\n",
    "    clf.fit(... your code here ...)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    report = classification_report(... your code here ...,  target_names=CORPORA, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": report[\"accuracy\"],\n",
    "        \"Macro F1\": report[\"macro avg\"][\"f1-score\"],\n",
    "        \"Weighted F1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        \"Train Time (s)\": train_time\n",
    "    }\n",
    "\n",
    "# Define classifiers to compare\n",
    "classifiers = [\n",
    "    (\"Multinomial NB\", MultinomialNB()),\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    (\"Linear SVM\", LinearSVC(random_state=42, max_iter=2000)),\n",
    "]\n",
    "\n",
    "# Evaluate all classifiers\n",
    "results = []\n",
    "for name, clf in tqdm(classifiers, desc=\"Training classifiers\"):\n",
    "    result = evaluate_classifier(name, clf, X_train, X_test, y_train, y_test)\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index(\"Model\")\n",
    "results_df.style.format({\n",
    "    \"Accuracy\": \"{:.2%}\",\n",
    "    \"Macro F1\": \"{:.2%}\",\n",
    "    \"Weighted F1\": \"{:.2%}\",\n",
    "    \"Train Time (s)\": \"{:.3f}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2dba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Visualize the classifier comparison results\n",
    "\n",
    "# Solution:\n",
    "# Visualize comparison - using Macro F1 (better for imbalanced data!)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Accuracy comparison (with caveat)\n",
    "results_df[\"Accuracy\"].plot(kind=\"barh\", ax=axes[0], color=\"steelblue\")\n",
    "axes[0].set_xlabel(\"Accuracy\")\n",
    "axes[0].set_title(\"Accuracy (‚ö†Ô∏è inflated by majority class)\")\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Macro F1 comparison (fairer metric!)\n",
    "results_df[\"Macro F1\"] ... your code here ...\n",
    "\n",
    "# Training time comparison\n",
    "results_df[\"Train Time (s)\"] ... your code here ...\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä For imbalanced datasets like ours, Macro F1 is more reliable than Accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1180fa",
   "metadata": {},
   "source": [
    "### üí° Classifier Comparison Insights\n",
    "\n",
    "**Naive Bayes:**\n",
    "- ‚úÖ Fastest to train (great for large datasets)\n",
    "- ‚úÖ Works well with sparse, high-dimensional data\n",
    "- ‚ùå Assumes features are independent (often violated)\n",
    "\n",
    "**Logistic Regression:**\n",
    "- ‚úÖ Good balance of speed and accuracy\n",
    "- ‚úÖ Interpretable (coefficients show feature importance)\n",
    "- ‚úÖ Regularization prevents overfitting\n",
    "\n",
    "**Linear SVM:**\n",
    "- ‚úÖ Often best accuracy on text data\n",
    "- ‚úÖ Works well in high dimensions\n",
    "- ‚ùå Slower to train than NB\n",
    "- ‚ùå Less interpretable\n",
    "\n",
    "**Rule of thumb:** Start with Naive Bayes for quick experiments, use Logistic Regression for interpretability, SVM for maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65a534",
   "metadata": {},
   "source": [
    "## Part 7: N-gram Experiments\n",
    "\n",
    "**The Problem with Unigrams (single words):**\n",
    "- \"not good\" ‚Üí [\"not\", \"good\"] ‚Üí Loses the negation!\n",
    "- \"machine learning\" ‚Üí [\"machine\", \"learning\"] ‚Üí Loses the phrase meaning!\n",
    "\n",
    "**N-grams capture word sequences:**\n",
    "```\n",
    "Unigrams (1):  [\"the\", \"robot\", \"is\", \"good\"]\n",
    "Bigrams (2):   [\"the robot\", \"robot is\", \"is good\"]\n",
    "Trigrams (3):  [\"the robot is\", \"robot is good\"]\n",
    "```\n",
    "\n",
    "**Trade-off:** More n-grams = more features = slower & risk of overfitting.\n",
    "\n",
    "**üöß TODO:** Compare unigram vs unigram+bigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Compare unigrams, bigrams, and uni+bigrams\n",
    "# Compare different n-gram configurations\n",
    "\n",
    "# Solution:\n",
    "ngram_configs = [\n",
    "    (\"Unigrams (1,1)\", (1, 1)),\n",
    "    (\"Uni+Bigrams (1,2)\", (1, 2)),\n",
    "    (\"Bigrams only (2,2)\", (2, 2)),\n",
    "]\n",
    "\n",
    "ngram_results = []\n",
    "\n",
    "for name, ngram_range in tqdm(ngram_configs, desc=\"Testing n-grams\"):\n",
    "    # Create vectorizer with specific n-gram range\n",
    "    vec = TfidfVectorizer(max_features=5000, stop_words=\"english\", ngram_range=ngram_range)\n",
    "    X_ngram = vec.fit_transform(df[\"text\"])\n",
    "    \n",
    "    # Split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_ngram, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train Naive Bayes\n",
    "    clf = ... your code here ...\n",
    "    .. your code here ...\n",
    "    \n",
    "    acc = (y_pred == y_te).mean()\n",
    "    n_features = X_ngram.shape[1]\n",
    "    \n",
    "    ngram_results.append({\n",
    "        \"N-gram Config\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Num Features\": n_features\n",
    "    })\n",
    "\n",
    "ngram_df = pd.DataFrame(ngram_results)\n",
    "print(ngram_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e3404",
   "metadata": {},
   "source": [
    "### üîç N-gram Results Analysis\n",
    "\n",
    "**Surprising result:** Unigrams alone perform best (98.7%)!\n",
    "\n",
    "| Config | Accuracy | Why? |\n",
    "|--------|----------|------|\n",
    "| **Unigrams** | 98.7% | Single words are enough to distinguish these categories |\n",
    "| **Uni+Bigrams** | 98.3% | Slightly worse ‚Äî bigrams add noise without adding signal |\n",
    "| **Bigrams only** | 90.4% | Much worse ‚Äî loses important single-word features |\n",
    "\n",
    "**Why don't bigrams help here?**\n",
    "\n",
    "1. **Categories are already easy to separate** ‚Äî Single distinctive words like \"robot\", \"wood\", \"myth\" are sufficient. Adding \"arduino uno\" doesn't help when \"arduino\" alone is already 100% robotics.\n",
    "\n",
    "2. **Feature budget is fixed** ‚Äî With `max_features=5000`, adding bigrams means removing some unigrams. We lose \"arduino\" to make room for \"arduino uno\".\n",
    "\n",
    "3. **Bigrams are sparser** ‚Äî Each bigram appears in fewer documents, making them less reliable for classification.\n",
    "\n",
    "**When DO bigrams help?**\n",
    "- Sentiment analysis: \"not good\" vs \"good\"\n",
    "- Phrase-based topics: \"machine learning\", \"climate change\"\n",
    "- Disambiguation: \"apple pie\" vs \"apple stock\"\n",
    "\n",
    "**Key insight:** More features ‚â† better. Match your representation to your task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f596cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example bigrams from our corpus with their frequencies\n",
    "bigram_vec = TfidfVectorizer(ngram_range=(2, 2), max_features=100, stop_words=\"english\")\n",
    "X_bigrams = bigram_vec.fit_transform(df[\"text\"])\n",
    "\n",
    "# Get bigram names and their total TF-IDF scores across all documents\n",
    "bigram_names = bigram_vec.get_feature_names_out()\n",
    "bigram_scores = X_bigrams.sum(axis=0).A1  # Sum TF-IDF across all docs\n",
    "\n",
    "# Also get document frequency (how many docs contain each bigram)\n",
    "doc_freq = (X_bigrams > 0).sum(axis=0).A1\n",
    "\n",
    "# Create DataFrame and sort by frequency\n",
    "bigram_df = pd.DataFrame({\n",
    "    \"bigram\": bigram_names,\n",
    "    \"total_tfidf\": bigram_scores,\n",
    "    \"doc_count\": doc_freq\n",
    "}).sort_values(\"total_tfidf\", ascending=False)\n",
    "\n",
    "print(\"Top 30 Bigrams in Corpus (by total TF-IDF score):\")\n",
    "print(bigram_df.head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71254ac1",
   "metadata": {},
   "source": [
    "## Part 8: Feature Importance\n",
    "\n",
    "Which words are most predictive for each category? \n",
    "\n",
    "**How it works:**  \n",
    "Logistic Regression learns a **weight (coefficient)** for each word.\n",
    "- **Positive coefficient** ‚Üí word predicts this category\n",
    "- **Negative coefficient** ‚Üí word predicts other categories\n",
    "\n",
    "```\n",
    "P(mythology | text) ‚àù w_myth √ó count(\"myth\") + w_god √ó count(\"god\") + ...\n",
    "                      ‚Üë high weight = important for mythology\n",
    "```\n",
    "\n",
    "**üöß TODO:** Identify the top features for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07997434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Extract and display top predictive words per category\n",
    "# Train Logistic Regression to extract feature importance\n",
    "\n",
    "# Solution:\n",
    "# Using multi_class='ovr' for clearer per-class coefficients\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42, multi_class='ovr')\n",
    "lr_clf.fit(... your code here ...)\n",
    "\n",
    "# Get feature names from our TF-IDF vectorizer\n",
    "feature_names = tfidf_vec.get_feature_names_out()\n",
    "\n",
    "# Show top predictive words per category\n",
    "n_top = 10\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOP PREDICTIVE WORDS PER CATEGORY\")\n",
    "print(\"(Higher coefficient = stronger predictor for this category)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, category in enumerate(CORPORA):\n",
    "    # Each row of coef_ corresponds to one class\n",
    "    coefs = ... your code here ...\n",
    "    \n",
    "    # Get indices of top coefficients\n",
    "    top_indices = np.argsort(... your code here ...) ...\n",
    "    \n",
    "    print(f\"\\nüìö {category.upper()}:\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"   {feature_names[idx]:20s} (weight: {coefs[idx]:+.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Visualize feature importance as bar charts\n",
    "\n",
    "# Solution:\n",
    "# Visualize feature importance for all 6 categories\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, category in ... your code here ... :\n",
    "    ax = axes[i]\n",
    "    coefs = ... your code here ...\n",
    "    \n",
    "    # Get top 8 features\n",
    "    top_indices = np.argsort(coefs)[-8:]\n",
    "    top_words = [feature_names[idx] for idx in top_indices]\n",
    "    top_scores = [coefs[idx] for idx in top_indices]\n",
    "    \n",
    "    # Plot horizontal bars\n",
    "    colors = ['green' if s > 0 else 'red' for s in top_scores]\n",
    "    ax.barh(top_words, top_scores, color=plt.cm.tab10(i))\n",
    "    ax.set_title(f\"{category}\")\n",
    "    ax.set_xlabel(\"Coefficient (weight)\")\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Top Predictive Words per Category\", y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56066852",
   "metadata": {},
   "source": [
    "### 8.1 Word Clouds: Visualizing Category Vocabulary\n",
    "\n",
    "Word clouds provide an intuitive visual representation of the most important words per category.\n",
    "The size of each word reflects its coefficient weight (importance for prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud visualization of top predictive words per category\n",
    "# Using coefficient weights as word sizes\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, category in enumerate(CORPORA):\n",
    "    ax = axes[i]\n",
    "    coefs = lr_clf.coef_[i]\n",
    "    \n",
    "    # Create word->weight dictionary (only positive weights)\n",
    "    word_weights = {}\n",
    "    for idx, weight in enumerate(coefs):\n",
    "        if weight > 0:\n",
    "            word_weights[feature_names[idx]] = weight\n",
    "    \n",
    "    # Generate word cloud\n",
    "    if word_weights:\n",
    "        wc = WordCloud(\n",
    "            width=400, height=300,\n",
    "            background_color='white',\n",
    "            colormap=f'tab10',\n",
    "            max_words=50,\n",
    "            prefer_horizontal=0.7\n",
    "        ).generate_from_frequencies(word_weights)\n",
    "        \n",
    "        ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.set_title(f\"{category.upper()}\", fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Word Clouds: Top Predictive Words per Category\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9b691",
   "metadata": {},
   "source": [
    "## Part 9: Hyperparameter Tuning\n",
    "\n",
    "The performance of text classifiers depends heavily on vectorization parameters. Let's systematically explore:\n",
    "- **`max_features`**: How many vocabulary words to keep\n",
    "- **`min_df`**: Minimum document frequency (ignore rare words)\n",
    "\n",
    "**üöß TODO:** Experiment with different hyperparameters and observe their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Experiment with different max_features values\n",
    "# Experiment 1: Effect of max_features\n",
    "\n",
    "# Solution:\n",
    "max_features_options = [100, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000]\n",
    "max_feat_results = []\n",
    "\n",
    "for max_feat in tqdm(max_features_options, desc=\"Testing max_features\"):\n",
    "    vec = TfidfVectorizer(max_features=max_feat, stop_words=\"english\")\n",
    "    X_exp = vec.fit_transform(df[\"text\"])\n",
    "    \n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(... your code here ..., random_state=42)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(... your code here ...)\n",
    "    acc = (... your code here ... == ... your code here ...).mean()\n",
    "    \n",
    "    max_feat_results.append({\"max_features\": max_feat, \"accuracy\": acc})\n",
    "\n",
    "max_feat_df = pd.DataFrame(max_feat_results)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(max_feat_df[\"max_features\"], max_feat_df[\"accuracy\"], marker=\"o\", linewidth=2)\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Effect of Vocabulary Size on Classification Accuracy\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(max_feat_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Effect of min_df (minimum document frequency)\n",
    "# min_df filters out words that appear in fewer than N documents\n",
    "# \n",
    "# ‚ö†Ô∏è NOTE: With max_features=5000, min_df has NO effect until it removes\n",
    "# enough rare words that the vocabulary drops below 5000!\n",
    "# That's why accuracy stays constant at 0.987 until min_df=200.\n",
    "# \n",
    "# The lesson: max_features already removes rare words (keeps only top 5000).\n",
    "# min_df becomes useful when you DON'T cap max_features, or when you want\n",
    "# to explicitly filter domain-specific rare terms.\n",
    "\n",
    "min_df_options = [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "min_df_results = []\n",
    "\n",
    "for min_df in tqdm(min_df_options, desc=\"Testing min_df\"):\n",
    "    vec = TfidfVectorizer(max_features=5000, min_df=min_df, stop_words=\"english\")\n",
    "    X_exp = vec.fit_transform(df[\"text\"])\n",
    "    \n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_exp, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    acc = (clf.predict(X_te) == y_te).mean()\n",
    "    n_features = X_exp.shape[1]\n",
    "    \n",
    "    min_df_results.append({\"min_df\": min_df, \"accuracy\": acc, \"num_features\": n_features})\n",
    "\n",
    "min_df_df = pd.DataFrame(min_df_results)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax1.plot(min_df_df[\"min_df\"], min_df_df[\"accuracy\"], marker=\"o\", color=\"steelblue\", linewidth=2, label=\"Accuracy\")\n",
    "ax1.set_xlabel(\"min_df\")\n",
    "ax1.set_ylabel(\"Accuracy\", color=\"steelblue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"steelblue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(min_df_df[\"min_df\"], min_df_df[\"num_features\"], marker=\"s\", color=\"coral\", linewidth=2, label=\"Num Features\")\n",
    "ax2.set_ylabel(\"Number of Features\", color=\"coral\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"coral\")\n",
    "\n",
    "plt.title(\"Effect of min_df on Accuracy and Feature Count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(min_df_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Effect of sublinear_tf (log transformation)\n",
    "# sublinear_tf replaces tf with 1 + log(tf) - reduces impact of high-frequency terms\n",
    "sublinear_options = [False, True]\n",
    "sublinear_results = []\n",
    "\n",
    "for sublinear in sublinear_options:\n",
    "    vec = TfidfVectorizer(max_features=5000, sublinear_tf=sublinear, stop_words=\"english\")\n",
    "    X_exp = vec.fit_transform(df[\"text\"])\n",
    "    # üöß TODO:\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(... your code here ..., random_state=42)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    acc = (clf.predict(X_te) == y_te).mean()\n",
    "    \n",
    "    sublinear_results.append({\"sublinear_tf\": sublinear, \"accuracy\": acc})\n",
    "\n",
    "print(\"Effect of sublinear_tf (logarithmic term frequency):\")\n",
    "print(pd.DataFrame(sublinear_results).to_string(index=False))\n",
    "print(\"\\nsublinear_tf=True uses: 1 + log(tf) instead of raw tf\")\n",
    "print(\"This reduces the dominance of words that appear many times in one document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934efe4",
   "metadata": {},
   "source": [
    "### üîç Sublinear TF Analysis\n",
    "\n",
    "**Result:** `sublinear_tf=True` slightly *decreases* accuracy (98.5% vs 98.7%).\n",
    "\n",
    "**Why?** Sublinear scaling uses `1 + log(tf)` instead of raw term frequency, which:\n",
    "- Dampens the impact of words that appear many times in one document\n",
    "- Useful when a word appearing 10√ó vs 100√ó doesn't mean 10√ó more relevant\n",
    "\n",
    "**When sublinear_tf helps:**\n",
    "- Long documents where some words repeat excessively\n",
    "- Reducing the influence of \"bursty\" words (words that cluster in specific documents)\n",
    "\n",
    "**Why it doesn't help here:**\n",
    "- Our documents are relatively short StackExchange posts\n",
    "- Word repetition is already limited\n",
    "- The baseline TF-IDF already works well for this corpus\n",
    "\n",
    "**Key insight:** Sublinear scaling is a form of **dampening** ‚Äî useful when raw counts are too noisy, but may lose signal when counts are already meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3877cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Effect of norm (L1 vs L2 normalization)\n",
    "# Normalization determines how document vectors are scaled\n",
    "norm_options = [None, \"l1\", \"l2\"]\n",
    "norm_results = []\n",
    "\n",
    "for norm in norm_options:\n",
    "    vec = TfidfVectorizer(max_features=5000, norm=norm, stop_words=\"english\")\n",
    "    X_exp = vec.fit_transform(df[\"text\"])\n",
    "    # üöß TODO:\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(... your code here ..., random_state=42)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    acc = ... your code here ...\n",
    "    \n",
    "    # Show vector magnitude for first document\n",
    "    sample_vec = X_exp[0].toarray().flatten()\n",
    "    l1_norm = np.abs(sample_vec).sum()\n",
    "    l2_norm = np.sqrt((sample_vec ** 2).sum())\n",
    "    \n",
    "    norm_results.append({\"norm\": str(norm), \"accuracy\": acc, \"sample_L1\": l1_norm, \"sample_L2\": l2_norm})\n",
    "\n",
    "print(\"Effect of vector normalization:\")\n",
    "print(pd.DataFrame(norm_results).to_string(index=False))\n",
    "print(\"\\nL2 norm (default): ||v||‚ÇÇ = 1 (unit vectors, good for cosine similarity)\")\n",
    "print(\"L1 norm: ||v||‚ÇÅ = 1 (probabilities sum to 1)\")\n",
    "print(\"None: raw TF-IDF weights (longer documents have larger vectors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fccb20",
   "metadata": {},
   "source": [
    "### üîç Normalization Analysis\n",
    "\n",
    "**Results:** L2 normalization (default) performs best, no normalization is close behind, but **L1 performs much worse!**\n",
    "\n",
    "| Norm | Accuracy | Effect |\n",
    "|------|----------|--------|\n",
    "| **L2** | 98.7% | Unit vectors (‚Äñv‚Äñ‚ÇÇ = 1) |\n",
    "| **None** | 98.2% | Raw TF-IDF weights |\n",
    "| **L1** | 78.3% | Probabilities sum to 1 (‚Äñv‚Äñ‚ÇÅ = 1) |\n",
    "\n",
    "**Why L1 normalization hurts Naive Bayes so badly:**\n",
    "- L1 normalization forces all feature values to sum to 1, treating them like probabilities\n",
    "- This **compresses** the dynamic range of TF-IDF scores dramatically (sample_L2 drops from 66.8 to 0.2!)\n",
    "- Multinomial NB multiplies many small probabilities ‚Üí numerical instability\n",
    "- The distinctions between important and unimportant words are washed out\n",
    "\n",
    "**Why L2 slightly beats no normalization:**\n",
    "- L2 creates unit vectors, making documents comparable regardless of length\n",
    "- Without normalization, longer documents have larger magnitudes, which can bias predictions\n",
    "\n",
    "**Key insight:** For Naive Bayes with TF-IDF, use L2 normalization (the default). L1 normalization is a poor choice here because it over-compresses the feature values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: Classifier regularization strength (C parameter)\n",
    "# For Logistic Regression: C controls regularization (smaller C = more regularization)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vec = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "X_exp = vec.fit_transform(df[\"text\"])\n",
    "# üöß TODO:\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(... your code here ..., random_state=42)\n",
    "\n",
    "C_options = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "C_results = []\n",
    "\n",
    "for C in tqdm(C_options, desc=\"Testing C values\"):\n",
    "    clf = LogisticRegression(C=C, max_iter=1000, random_state=42)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    acc = ... your code here ...\n",
    "    \n",
    "    # Count non-zero coefficients (sparsity)\n",
    "    n_nonzero = (np.abs(clf.coef_) > 1e-6).sum()\n",
    "    \n",
    "    C_results.append({\"C\": C, \"accuracy\": acc, \"nonzero_coefs\": n_nonzero})\n",
    "\n",
    "C_df = pd.DataFrame(C_results)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "ax1.semilogx(C_df[\"C\"], C_df[\"accuracy\"], marker=\"o\", color=\"steelblue\", linewidth=2)\n",
    "ax1.set_xlabel(\"C (regularization: lower = stronger)\")\n",
    "ax1.set_ylabel(\"Accuracy\", color=\"steelblue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"steelblue\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.semilogx(C_df[\"C\"], C_df[\"nonzero_coefs\"], marker=\"s\", color=\"coral\", linewidth=2)\n",
    "ax2.set_ylabel(\"Non-zero coefficients\", color=\"coral\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"coral\")\n",
    "\n",
    "plt.title(\"Logistic Regression: Regularization Strength (C)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(C_df.to_string(index=False))\n",
    "print(\"\\nLow C = strong L2 regularization ‚Üí many coefficients shrink toward 0\")\n",
    "print(\"High C = weak regularization ‚Üí model fits training data more closely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e88c9f",
   "metadata": {},
   "source": [
    "### üîç Regularization Strength (C) Analysis\n",
    "\n",
    "**Results:** Performance is stable across a wide range of C values, but extreme regularization (C=0.001) hurts accuracy.\n",
    "\n",
    "| C Value | Regularization | Effect |\n",
    "|---------|----------------|--------|\n",
    "| **0.001** | Very strong | Underfitting ‚Äî too many coefficients shrunk to zero |\n",
    "| **0.01‚Äì1.0** | Moderate | Good balance ‚Äî prevents overfitting |\n",
    "| **10‚Äì100** | Weak | Slightly higher variance, but still works well |\n",
    "\n",
    "**What C controls:**\n",
    "- **Low C** = Strong L2 regularization ‚Üí coefficients shrink toward zero ‚Üí simpler model\n",
    "- **High C** = Weak regularization ‚Üí model fits training data more closely ‚Üí risk of overfitting\n",
    "\n",
    "**Why the \"sweet spot\" is broad:**\n",
    "- Our dataset is large (~50K documents) relative to features (5000)\n",
    "- Text classification with TF-IDF is already well-regularized by the high dimensionality\n",
    "- The model has enough data to learn without needing strong regularization\n",
    "\n",
    "**The non-zero coefficient plot shows:**\n",
    "- With very low C, many coefficients are effectively zero (sparse model)\n",
    "- As C increases, more features are used (denser model)\n",
    "\n",
    "**Key insight:** For text classification, the default C=1.0 usually works well. Only tune C if you see overfitting (train >> test accuracy) or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1405c",
   "metadata": {},
   "source": [
    "## Part 10: Cross-Validation for Robust Evaluation\n",
    "\n",
    "**The Problem with a Single Train/Test Split:**\n",
    "- Results depend on which samples ended up in test set\n",
    "- With a different `random_state`, we might get different accuracy!\n",
    "- We need a more reliable estimate of model performance\n",
    "\n",
    "**Cross-Validation (CV):**\n",
    "```\n",
    "Fold 1: [TEST] [train] [train] [train] [train]\n",
    "Fold 2: [train] [TEST] [train] [train] [train]\n",
    "Fold 3: [train] [train] [TEST] [train] [train]\n",
    "Fold 4: [train] [train] [train] [TEST] [train]\n",
    "Fold 5: [train] [train] [train] [train] [TEST]\n",
    "                    ‚Üì\n",
    "         Average scores across all folds\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Every sample is tested exactly once\n",
    "- Get mean ¬± std of performance (confidence interval!)\n",
    "- More reliable for model comparison\n",
    "\n",
    "**üöß TODO:** Implement 5-fold cross-validation to compare classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f37877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöß TODO: Implement cross-validation for classifier comparison\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Solution:\n",
    "# Compare classifiers with 5-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_classifiers = [\n",
    "    (\"Multinomial NB\", MultinomialNB()),\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    (\"Linear SVM\", LinearSVC(random_state=42, max_iter=2000)),\n",
    "]\n",
    "\n",
    "cv_results = []\n",
    "for name, clf in tqdm(cv_classifiers, desc=\"Cross-validating\"):\n",
    "    scores = ... your code here ...\n",
    "    cv_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Mean Accuracy\": scores.mean(),\n",
    "        \"Std\": scores.std(),\n",
    "        \"Min\": scores.min(),\n",
    "        \"Max\": scores.max(),\n",
    "        \"All Folds\": scores\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).set_index(\"Model\")\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(cv_df[[\"Mean Accuracy\", \"Std\", \"Min\", \"Max\"]].to_string())\n",
    "\n",
    "# Visualize with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "means = [r[\"Mean Accuracy\"] for r in cv_results]\n",
    "stds = [r[\"Std\"] for r in cv_results]\n",
    "names = [r[\"Model\"] for r in cv_results]\n",
    "\n",
    "bars = ax.barh(names, means, xerr=stds, capsize=5, color=plt.cm.tab10.colors[:3])\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "ax.set_title(\"Cross-Validation: Mean Accuracy ¬± Std Dev\")\n",
    "ax.set_xlim(0.9, 1.0)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    ax.text(mean + std + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "            f'{mean:.1%} ¬± {std:.1%}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59b6b1",
   "metadata": {},
   "source": [
    "### üîç Interpreting Cross-Validation Results\n",
    "\n",
    "**Our Results:**\n",
    "\n",
    "| Model | Mean Accuracy | Std Dev | Interpretation |\n",
    "|-------|---------------|---------|----------------|\n",
    "| **Linear SVM** | 99.5% | ¬±0.07% | ü•á Best and most stable |\n",
    "| **Logistic Regression** | 99.3% | ¬±0.05% | ü•à Very close, lowest variance |\n",
    "| **Multinomial NB** | 98.6% | ¬±0.14% | ü•â Good, but slightly more variable |\n",
    "\n",
    "**Analysis:** # üöß TODO:\n",
    "\n",
    "1. **SVM ... your observations here ...** \n",
    "\n",
    "2. **Naive Bayes is fast but ... your observations here ...**\n",
    "\n",
    "3. **All models have tiny ... your observations here ...** ‚Äî This means ... your observations here ...\n",
    "\n",
    "**What to look for in general:**\n",
    "\n",
    "1. **Mean Accuracy** ‚Äî The average performance across all folds. More reliable than a single split!\n",
    "\n",
    "2. **Standard Deviation** ‚Äî How much performance varies across folds. Low std = stable model.\n",
    "\n",
    "3. **Overlapping error bars** ‚Äî If error bars overlap, the difference between models may not be statistically significant.\n",
    "\n",
    "**Key insight:** Cross-validation gives us confidence in our results. When comparing models, look at both mean AND variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c4ebb2",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### üìö What we learned:\n",
    "\n",
    "| Topic | Key Insight |\n",
    "|-------|-------------|\n",
    "| **Vectorization** | Text must be converted to numbers. BoW counts words, TF-IDF weighs by importance. |\n",
    "| **Sparse Matrices** | Text vectors are mostly zeros ‚Üí store efficiently with sparse matrices. |\n",
    "| **TF-IDF** | Downweights common words, upweights distinctive ones. Better for classification! |\n",
    "| **Classifiers** | NB is fast, LogReg is interpretable, SVM often most accurate. |\n",
    "| **Evaluation** | ‚ö†Ô∏è Use **Macro F1** for imbalanced datasets! Accuracy is inflated by majority class. |\n",
    "| **N-grams** | Can capture phrases, but not always helpful. Match features to task. |\n",
    "| **Feature Importance** | Model coefficients reveal which words drive predictions. |\n",
    "| **Hyperparameters** | `max_features`, `min_df`, `C` all impact results significantly. |\n",
    "| **Cross-Validation** | More reliable than single train/test split. Always use for model comparison! |\n",
    "\n",
    "### üéØ Practical Tips:\n",
    "\n",
    "1. **Start simple** ‚Äî Naive Bayes + TF-IDF is a strong baseline\n",
    "2. **Inspect your data** ‚Äî Class balance, text lengths, weird characters\n",
    "3. **Check class balance** ‚Äî With imbalanced data, prefer Macro F1 over accuracy\n",
    "4. **Try multiple classifiers** ‚Äî They have different strengths\n",
    "5. **Use cross-validation** ‚Äî Single splits can be misleading\n",
    "6. **Analyze errors** ‚Äî Misclassifications reveal model limitations\n",
    "\n",
    "### üöÄ Next steps to explore:\n",
    "- **Character n-grams** ‚Äî Robust to typos and morphological variations\n",
    "- **LSA/SVD** ‚Äî Dimensionality reduction on TF-IDF (latent semantic analysis)\n",
    "- **Word embeddings** ‚Äî Dense representations (Word2Vec, FastText) in Notebook 3\n",
    "- **Deep learning** ‚Äî Neural text classifiers in later notebooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
